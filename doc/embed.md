Vector semantics are models in which researcher use a formal mathematical structure (i.e. vectors) to represent how lexical meanings of words, represented by a vector, reside in a vector space. The vectors representing each words also encode, to some extent, their mutual semantic relationshis in that space. This general approach, while being a hot topic in recent years (cite some studies, such as LSA, LDA, mikolov, ElMo), could be traced back to the early 20th century (cite Flirth and ...). The idea was to explore the cooccurence of the words in context (sentences, or a groups of preceeding and following words), and use the context to determine the "location" of a word vector in semantic space, where thus location could best reflect the relationships with other words. 

While models of vector semantics enjoyed great successes in various NLP tasks, even were indispensible constructs in virtually all deep learning models, challenges emerged when they came to WordNet. WordNet, as a lexical resource of word senses and linguistic knowledges, make intricate distinctions on word senses and the synsets among them. However, vector semantics models had a major limitation of meaning conflation deficiency (cite Camacho-Collados, 2018), namely they conflate multiple meanings of a word (lemma) into one representation. For example, in word2vec model (cite Mikolov), vectors of target word were constructed through the task of predicting the target word with surrounding word vectors (continous bag of words, CBOW), or, conversely, predicting surrounding words with the target word vectors (skip gram). Different word contexts were independent samples in training, they are not explicitly used by the model. The resulting word vectors were therefore undifferentiated representations of word senses.

Other models have the potential to accommodate, or even represent, word senses information, but not without caveats. For example, latent dirichlet allocation (LDA, cite Griffiths) represented meanings of each word as a probability distribution over different topics. It could describe each word senses as mixtures of different topic components. But the problems remains on how to relate latent topics on the word senses. Other endeavors relies on a sense disambiguated corpus (cite Iacobacci senseembed), and inferred the sense vectors through the disambiguated context. But this approach required a mature word sense disambiguation (WSD) algorithm or manually annotated sense-disambiguated corpus with given sets of word sense distinctions. Chinese WSD is an active and productive research topic, but the word sense disambiguation on CWN word senses remained an challenging task.

Instead of relying on sense-disambiguated corpus, recent models tried to incorporate word context into deep learning models and construct contextualized vectors (cite Peters ELMo paper and McCann-2018). Inspired by the deep learning models in computer vision, these models tried to represent word contexts as an abastract information built upon the basic word embeddings in a language modeling task. Specifically, a model was trained to predict the next word in a sentence, basing on the words previously seen. The models used word vectors as input, but stacked upon embeddings layers (i.e. word vectors) were deep layers tried to encode the context information. The outputs of these deep layers were used to complete the prediction task in training; and additionally, they represented the context vectors the words occured in. Recent deep learning researches provided multiple choices of such layers, like bidirectional LSTM used in ELMo and decoder transformer used in OpenAI transformer (cite Vasawani). These models, instead of treating each word as a static vector, could generate a contextualized vectors for each word in any given contexts. However, as these models were trained on language modeling tasks, the model only made use of either preceeding or succeeding word contexts to build context vectors. 

Bidirectional-encoder representation (BERT, cite Devlin) used different task in order to train models to make use of surrounding word contexts to generate context vectors. As other contextualized vector models, BERT also use word vector as its input, but the deep layers stacked upon them were layers of encoder transformers (cite Vasawani). In order to allow encoder to consider the surrounding word contexts in the same time, but without peeking into the predicting targets, BERT used the cloze task in training stage. In the cloze task, each word in the whole sentence was available to model, with only the cloze word (the target) masked out. The model was to learn how to construct a context vector with the surrounding words, and predict the cloze word with the context vectors. The contextualized vector trained on this model have wide range of applicability. It had been shown the model, without substantial modification, achieved superior performance on NLP tasks such as question answering and language inference.

As models of contextualized vectors encodes the context into models, it is interesting to investigate further whether the context vectors "learned" from the model could help researchers to identify or even manipulate the word senses defined in WordNet, CWN specifically. The purpose of present paper was as follows: (1) examine the context vector obtained by contextualzed vector models (i.e. BERT) indeed reflected the word sense distinction made available in CWN. (2) Use the contextualized vector for each sense as sense vectors, along with the heuristics rule for Chinese word morphology, generate possible hypernymy-hyponymy relationships in the CWN. (3) Evaluate the qualities of semantic relationship suggested by BERT and heuristics.

